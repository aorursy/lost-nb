#!/usr/bin/env python
# coding: utf-8



get_ipython().system('curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py')
get_ipython().system('python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev')









get_ipython().run_cell_magic('writefile', 'XLMROBERTA_TPU.py', '\nimport argparse\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import DataLoader, Dataset\n#from apex import amp\nimport random\nimport re\nimport json\nfrom transformers import ( \n    BertTokenizer, \n    AdamW, \n    BertModel, \n    BertForPreTraining,\n    BertConfig,\n    get_linear_schedule_with_warmup,\n    BertTokenizerFast,\n    RobertaModel,\n    RobertaTokenizerFast,\n    RobertaConfig,\n    AlbertTokenizer,\n    AlbertConfig,\n    AlbertModel\n)\nimport transformers\n\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\nimport sentencepiece as spm\n#import sentencepiece_pb2\n\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\nimport sys\nsys.path.insert(0, "../input/sentencepiece-pb2/")\nimport sentencepiece_pb2\n\n\ndef to_list(tensor):\n    return tensor.detach().cpu().tolist()\n\nclass AverageMeter(object):\n    """Computes and stores the average and current values"""\n    def __init__(self):\n        self.reset()\n    \n    def __init__(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef get_position_accuracy(logits, labels):\n    predictions = np.argmax(F.softmax(logits, dim=1).cpu().data.numpy(), axis=1)\n    labels = labels.cpu().data.numpy()\n    total_num = 0\n    sum_correct = 0\n    for i in range(len(labels)):\n        if labels[i] >= 0:\n            total_num += 1\n            if predictions[i] == labels[i]:\n                sum_correct += 1\n    if total_num == 0:\n        total_num = 1e-7\n    return np.float32(sum_correct) / total_num, total_num\n\ndef jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / ((len(a) + len(b)) - len(c))\n\ndef calculate_jaccard_score(features_dict, start_logits, end_logits, tokenizer):\n\n    input_ids = to_list(features_dict["input_ids"])\n    #start_position = to_list(features_dict["start_position"])\n    #end_position = to_list(features_dict["end_position"])\n    tweet = features_dict["tweet"]\n    selected_text = features_dict["selected_text"]\n    sentiment = features_dict["sentiment"]\n    #offsets = features_dict["offsets"]\n\n    start_logits = np.argmax(F.softmax(start_logits, dim=1).cpu().data.numpy(), axis=1)\n    end_logits = np.argmax(F.softmax(end_logits, dim=1).cpu().data.numpy(), axis=1)\n\n    jac_list = []\n\n    for i in range(len(tweet)):\n\n        idx_start = start_logits[i]\n        idx_end = end_logits[i]\n        #offset = offsets[i]\n        input_id = input_ids[i]\n        tw = tweet[i]\n        target_st = selected_text[i]\n\n        if idx_end < idx_start:\n            idx_end = idx_start\n\n        """\n        filtered_output = ""\n        for ix in range(idx_start, idx_end):\n            filtered_output += tw[offset[ix][0]: offset[ix][1]]\n            if (ix+1) < len(offset) and offset[ix][1] < offset[ix+1][0]:\n                filtered_output += " "\n        """\n        filtered_output = tokenizer.decode(input_id[idx_start:idx_end+1], skip_special_tokens=True)\n\n        if sentiment[i] == "neutral" or len(tw.split()) < 2:\n            filtered_output = tw\n        \n        \n        jac = jaccard(target_st.strip(), filtered_output.strip())\n\n        jac_list.append(jac)\n\n    return np.mean(jac_list)\n\n########## Datapreparation & Dataset ###########\n\ndef find_start_and_end(tweet, selected_text):\n    len_st = len(selected_text)\n    start = None\n    end = None\n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n        if tweet[ind: ind+len_st] == selected_text:\n            start = ind\n            end = ind + len_st - 1\n            break\n    return start, end\n\ndef _is_whitespace(c):\n    if c == " " or c == "\\t" or c == "\\r" or c == "\\n" or ord(c) == 0x202F:\n        return True\n    return False\n\ndef whitespace_tokenizer(text):\n    """Runs basic whitespace cleaning and splitting on a piece of text."""\n    text = text.strip()\n    #print(text)\n    if not text:\n         return []\n    tokens = text.split()\n    return tokens\n\ndef _improve_answer_span(doc_tokens, input_start, input_end, tokenizer, orig_answer_text):\n    """Returns tokenized answer spans that better match the annotated answer."""\n    tok_answer_text = " ".join(tokenizer.tokenize(orig_answer_text))\n     \n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = " ".join(doc_tokens[new_start : (new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n    return (input_start, input_end)\n\nclass SentencePieceTokenizer:\n    def __init__(self, model_path):\n        self.sp = spm.SentencePieceProcessor()\n        self.sp.load(os.path.join(model_path))\n    \n    def encode(self, sentence):\n        spt = sentencepiece_pb2.SentencePieceText()\n        spt.ParseFromString(self.sp.encode_as_serialized_proto(sentence))\n        offsets = []\n        tokens = []\n        for piece in spt.pieces:\n            tokens.append(piece.id)\n            offsets.append((piece.begin, piece.end))\n        return tokens, offsets\n\ndef find_start_end_offsets(tweet, selected_text, start_position_character, tokenizer):\n    \n    start_position = 0\n    end_position = 0\n    \n    tweet_tokens = []\n    \n    char_to_word_offset = []\n    prev_is_whitespace = True\n    \n    # Split on whitespace so that different tokens may be attributed to their original position.\n    for c in tweet:\n        if _is_whitespace(c):\n            prev_is_whitespace = True\n        else:\n            if prev_is_whitespace:\n                tweet_tokens.append(c)\n            else:\n                tweet_tokens[-1] += c\n            prev_is_whitespace = False\n        char_to_word_offset.append(len(tweet_tokens) - 1)\n    \n    # Start and end positons only has a value during evalution.\n    if start_position_character is not None:\n        start_position = char_to_word_offset[start_position_character]\n        end_position = char_to_word_offset[\n                min(start_position_character + len(selected_text) - 1, len(char_to_word_offset) -1)                                  \n        ]\n        \n    tok_to_orig_index = []\n    orig_to_tok_index = []\n    all_tweet_tokens = []\n    \n    for (i, token) in enumerate(tweet_tokens):\n        orig_to_tok_index.append(len(all_tweet_tokens))\n        sub_tokens = tokenizer.tokenize(token)\n        for sub_token in sub_tokens:\n            tok_to_orig_index.append(i)\n            all_tweet_tokens.append(sub_token)\n    #print(orig_to_tok_index)\n    tok_start_position = orig_to_tok_index[start_position]\n    if end_position < len(tweet_tokens) - 1:\n        tok_end_position = orig_to_tok_index[end_position + 1] - 1\n    else:\n        tok_end_position = len(all_tweet_tokens) - 1\n    \n     \n    \n    (tok_start_position, tok_end_position) = _improve_answer_span(\n            all_tweet_tokens, tok_start_position, tok_end_position, tokenizer, selected_text\n        )\n    \n    return tok_start_position, tok_end_position\n\ndef process_with_offsets(args, tweet, selected_text, sentiment, tokenizer, spt):\n\n    start_index, end_index = find_start_and_end(tweet, selected_text)\n\n    char_targets = [0]*len(tweet)\n    if start_index != None and end_index != None:\n        for ct in range(start_index, end_index+1):\n            char_targets[ct] = 1\n    \n    encoded = tokenizer.encode_plus(\n                    sentiment,\n                    tweet,\n                    max_length=args.max_seq_len,\n                    pad_to_max_length=True,\n                    return_token_type_ids=True,\n                    #return_offsets_mapping=True\n                )\n    \n    len_sentences_pair_tokens = tokenizer.max_len - tokenizer.max_len_sentences_pair + 2\n\n    offsets = spt.encode(tweet)[-1]\n    encoded["offset_mapping"] = [(0,0)]*4 + offsets\n\n    target_idx = []\n    for j, (offset1, offset2) in enumerate(encoded["offset_mapping"]):\n        if j > 3:#(len_sentences_pair_tokens + 2) - 1 :\n            if sum(char_targets[offset1:offset2]) > 0:\n                target_idx.append(j)\n\n    #sp , ep = find_start_end_offsets(tweet, selected_text, start_index, tokenizer)\n    #print(sp, ep)\n    \n    \n    encoded["start_position"] = target_idx[0]\n    encoded["end_position"] = target_idx[-1]\n    encoded["tweet"] = tweet\n    encoded["selected_text"] = selected_text\n    encoded["sentiment"] = sentiment\n\n    return encoded\n\nclass TweetDataset:\n    def __init__(self, args, tokenizer, spt , df, mode="train", fold=0):\n        \n        self.mode = mode\n\n        if self.mode == "train":\n            df = df[~df.kfold.isin([fold])].dropna()\n            self.tweet = df.text.values\n            self.sentiment = df.sentiment.values\n            self.selected_text = df.selected_text.values\n        \n        elif self.mode == "valid":\n            df = df[df.kfold.isin([fold])].dropna()\n            self.tweet = df.text.values\n            self.sentiment = df.sentiment.values\n            self.selected_text = df.selected_text.values\n        \n        self.tokenizer = tokenizer\n        self.args = args\n        self.spt = spt\n    \n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n\n        tweet = str(self.tweet[item])\n        selected_text = str(self.selected_text[item])\n        sentiment = str(self.sentiment[item])\n        \n        features = process_with_offsets(\n                        args=self.args, \n                        tweet=tweet, \n                        selected_text=selected_text, \n                        sentiment=sentiment, \n                        tokenizer=self.tokenizer,\n                        spt=self.spt\n                    )\n        \n        return {\n            "input_ids":torch.tensor(features["input_ids"], dtype=torch.long),\n            "token_type_ids":torch.tensor(features["token_type_ids"], dtype=torch.long),\n            "attention_mask":torch.tensor(features["attention_mask"], dtype=torch.long),\n            #"offsets":features["offset_mapping"],\n            "start_position":torch.tensor(features["start_position"],dtype=torch.long),\n            "end_position":torch.tensor(features["end_position"], dtype=torch.long),\n\n            "tweet":features["tweet"],\n            "selected_text":features["selected_text"],\n            "sentiment":features["sentiment"]\n\n        }\n\nclass BertForQuestionAnswering(BertForPreTraining):\n    """\n    BERT model for QA\n\n    Parameters\n    ----------\n    config : transformers.BertConfig. Configuration class for BERT.\n\n    Returns\n    -------\n    start_logits : torch.Tensor with shape (batch_size, sequence_size)\n        Starting scores of each tokens.\n    end_logits : torch.Tenosr with shape (batch_size, sequence_size).\n        Ending scores of each tokens.\n    """\n\n    def __init__(self, config):\n        super(BertForQuestionAnswering, self).__init__(config)\n        self.bert = RobertaModel(config)\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            #token_type_ids=token_type_ids,\n                            #position_ids=position_ids,\n                            #head_mask=head_mask\n                        )\n        sequence_output = outputs[0]\n        #pooled_output = outputs[1]\n\n        # predict start & end position\n        qa_logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = qa_logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits\n\nclass TweetModel(transformers.BertPreTrainedModel):\n    def __init__(self, model_path, conf):\n        super(TweetModel, self).__init__(conf)\n        self.xlmroberta = transformers.XLMRobertaModel.from_pretrained(model_path, config=conf)\n        self.drop_out = nn.Dropout(0.1)\n        self.l0 = nn.Linear(768 * 2, 2) #768\n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n\n\n    def forward(self,input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n        _,_, out = self.xlmroberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids\n        )\n\n        out = torch.cat((out[-1], out[-2]), dim=-1)\n        #print(out.shape)\n        out = self.drop_out(out)\n        logits = self.l0(out)\n\n        start_logits, end_logits = logits.split(1, dim=-1)\n\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits\n\ndef reduce_fn(vals):\n    return sum(vals) / len(vals)\n\ndef loss_fn(preds, labels):\n    start_preds, end_preds = preds\n    start_labels, end_labels = labels\n\n    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n    return start_loss, end_loss\n\ndef train(args, train_loader, model, optimizer,scheduler, epoch, f):\n    total_loss = AverageMeter()\n    losses1 = AverageMeter() # start\n    losses2 = AverageMeter() # end\n    accuracies1 = AverageMeter() # start\n    accuracies2 = AverageMeter() # end\n\n    model.train()\n\n    t = tqdm(train_loader, disable=not xm.is_master_ordinal())\n    for step, d in enumerate(t):\n        \n        input_ids = d["input_ids"].to(args.device)\n        attention_mask = d["attention_mask"].to(args.device)\n        token_type_ids = d["token_type_ids"].to(args.device)\n        start_position = d["start_position"].to(args.device)\n        end_position = d["end_position"].to(args.device)\n\n        model.zero_grad()\n\n        logits1, logits2 = model(\n            input_ids=input_ids, \n            attention_mask=attention_mask, \n            token_type_ids=token_type_ids, \n            position_ids=None, \n            head_mask=None\n        )\n\n        y_true = (start_position, end_position)\n        loss1, loss2 = loss_fn((logits1, logits2), (start_position, end_position))\n        loss = loss1 + loss2\n\n        acc1, n_position1 = get_position_accuracy(logits1, start_position)\n        acc2, n_position2 = get_position_accuracy(logits2, end_position)\n\n        total_loss.update(loss.item(), n_position1)\n        losses1.update(loss1.item(), n_position1)\n        losses2.update(loss2.item(), n_position2)\n        accuracies1.update(acc1, n_position1)\n        accuracies2.update(acc2, n_position2)\n\n        \n        #optimizer.zero_grad()\n        #with amp.scale_loss(loss, optimizer) as scaled_loss:\n        #    scaled_loss.backward()\n        loss.backward()\n        xm.optimizer_step(optimizer)\n        scheduler.step()\n        print_loss = xm.mesh_reduce("loss_reduce", total_loss.avg, reduce_fn)\n        print_acc1 = xm.mesh_reduce("acc1_reduce", accuracies1.avg, reduce_fn)\n        print_acc2 = xm.mesh_reduce("acc2_reduce", accuracies2.avg, reduce_fn)\n        t.set_description(f"Train E:{epoch+1} - Loss:{print_loss:0.2f} - acc1:{print_acc1:0.2f} - acc2:{print_acc2:0.2f}")\n\n\n    log_ = f"Epoch : {epoch+1} - train_loss : {total_loss.avg} - \\n \\\n    train_loss1 : {losses1.avg} - train_loss2 : {losses2.avg} - \\n \\\n    train_acc1 : {accuracies1.avg} - train_acc2 : {accuracies2.avg}"\n\n    f.write(log_ + "\\n\\n")\n    f.flush()\n    \n    return total_loss.avg\n\ndef valid(args, valid_loader, model, tokenizer, epoch, f):\n    total_loss = AverageMeter()\n    losses1 = AverageMeter() # start\n    losses2 = AverageMeter() # end\n    accuracies1 = AverageMeter() # start\n    accuracies2 = AverageMeter() # end\n\n    jaccard_scores = AverageMeter()\n\n    model.eval()\n\n    with torch.no_grad():\n        t = tqdm(valid_loader, disable=not xm.is_master_ordinal())\n        for step, d in enumerate(t):\n            \n            input_ids = d["input_ids"].to(args.device)\n            attention_mask = d["attention_mask"].to(args.device)\n            token_type_ids = d["token_type_ids"].to(args.device)\n            start_position = d["start_position"].to(args.device)\n            end_position = d["end_position"].to(args.device)\n\n            logits1, logits2 = model(\n                input_ids=input_ids, \n                attention_mask=attention_mask, \n                token_type_ids=None, \n                position_ids=None, \n                head_mask=None\n            )\n\n            y_true = (start_position, end_position)\n            loss1, loss2 = loss_fn((logits1, logits2), (start_position, end_position))\n            loss = loss1 + loss2\n\n            acc1, n_position1 = get_position_accuracy(logits1, start_position)\n            acc2, n_position2 = get_position_accuracy(logits2, end_position)\n\n            total_loss.update(loss.item(), n_position1)\n            losses1.update(loss1.item(), n_position1)\n            losses2.update(loss2.item(), n_position2)\n            accuracies1.update(acc1, n_position1)\n            accuracies2.update(acc2, n_position2)\n\n            jac_score = calculate_jaccard_score(features_dict=d, start_logits=logits1, end_logits=logits2, tokenizer=tokenizer)\n\n            jaccard_scores.update(jac_score)\n\n            print_loss = xm.mesh_reduce("vloss_reduce", total_loss.avg, reduce_fn)\n            print_jac = xm.mesh_reduce("jac_reduce", jaccard_scores.avg, reduce_fn)\n\n            t.set_description(f"Eval E:{epoch+1} - Loss:{print_loss:0.2f} - Jac:{print_jac:0.2f}")\n\n    #print("Valid Jaccard Score : ", jaccard_scores.avg)\n    log_ = f"Epoch : {epoch+1} - valid_loss : {total_loss.avg} - \\n\\\n    valid_loss1 : {losses1.avg} - \\valid_loss2 : {losses2.avg} - \\n\\\n    valid_acc1 : {accuracies1.avg} - \\valid_acc2 : {accuracies2.avg} "\n\n    f.write(log_ + "\\n\\n")\n    f.flush()\n    \n    return jaccard_scores.avg\n\n\ndef main():\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument("--local_rank", type=int, default=-1, help="local_rank for distributed training on gpus")\n    parser.add_argument("--max_seq_len", type=int, default=192)\n    parser.add_argument("--fold_index", type=int, default=0)\n    parser.add_argument("--learning_rate", type=float, default=0.00002)\n    parser.add_argument("--epochs", type=int, default=5)\n    parser.add_argument("--batch_size", type=int, default=16)\n    parser.add_argument("--model_path", type=str, default="roberta-base")\n    parser.add_argument("--output_dir", type=str, default="")\n    parser.add_argument("--exp_name", type=str, default="")\n    parser.add_argument("--spt_path", type=str, default="")\n\n    args = parser.parse_args()\n\n    # Setting seed\n    seed = 42\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n     # hperparameters\n    #args.max_seq_len = 192\n    #args.learning_rate = 0.00002\n    #args.batch_size = 16\n    #args.epochs = 5\n    #args.fold_index = 0\n\n    #model_path = "../huggingface_pretrained/bert-base-uncased/"\n    model_path = args.model_path\n    config = transformers.XLMRobertaConfig.from_pretrained(model_path)\n    config.output_hidden_states = True\n    tokenizer = transformers.XLMRobertaTokenizer.from_pretrained(model_path, do_lower_case=True)\n    #model = BertForQuestionAnswering.from_pretrained(model_path, config=config)\n    MX = TweetModel(model_path, config)\n\n    spt = SentencePieceTokenizer(args.spt_path)\n\n    train_df = pd.read_csv("../input/tweet-create-folds/train_3folds.csv")\n    #train_df = train_df[train_df.sentiment != "neutral"]\n    print(len(train_df))\n\n    args.save_path = os.path.join(args.output_dir, args.exp_name)\n\n    if not os.path.exists(args.save_path):\n        os.makedirs(args.save_path)\n\n    f = open(os.path.join(args.save_path, f"log_f_{args.fold_index}.txt"), "w")\n\n    def run():\n\n        args.device = xm.xla_device()\n        model = MX.to(args.device)\n\n\n        # DataLoaders\n        train_dataset = TweetDataset(\n            args=args,\n            df=train_df,\n            mode="train",\n            fold=args.fold_index,\n            tokenizer=tokenizer,\n            spt=spt\n        )\n        train_sampler = torch.utils.data.distributed.DistributedSampler(\n            train_dataset,\n            num_replicas=xm.xrt_world_size(),\n            rank=xm.get_ordinal(),\n            shuffle=True\n        )\n        train_loader = torch.utils.data.DataLoader(\n            train_dataset,\n            batch_size=args.batch_size,\n            sampler=train_sampler,\n            drop_last=True,\n            num_workers=2\n        )\n\n\n        valid_dataset = TweetDataset(\n            args=args,\n            df=train_df,\n            mode="valid",\n            fold=args.fold_index,\n            tokenizer=tokenizer,\n            spt=spt\n        )\n        valid_sampler = torch.utils.data.distributed.DistributedSampler(\n            valid_dataset,\n            num_replicas=xm.xrt_world_size(),\n            rank=xm.get_ordinal(),\n            shuffle=False\n        )\n        valid_loader = DataLoader(\n            valid_dataset,\n            batch_size=args.batch_size,\n            sampler=valid_sampler,\n            num_workers=1,\n            drop_last=False\n        )\n\n        #optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n        #model, optimizer = amp.initialize(model, optimizer, opt_level="O1",verbosity=0)\n\n        num_train_steps = int(len(train_df) / args.batch_size * args.epochs)\n    \n        param_optimizer = list(model.named_parameters())\n        no_decay = [\n            "bias",\n            "LayerNorm.bias",\n            "LayerNorm.weight"\n        ]\n        optimizer_parameters = [\n            {\n                \'params\': [\n                    p for n, p in param_optimizer if not any(\n                        nd in n for nd in no_decay\n                    )\n                ], \n            \'weight_decay\': 0.001\n            },\n            {\n                \'params\': [\n                    p for n, p in param_optimizer if any(\n                        nd in n for nd in no_decay\n                    )\n                ], \n                \'weight_decay\': 0.0\n            },\n        ]\n\n        num_train_steps = int(\n            len(train_df) / args.batch_size / xm.xrt_world_size() * args.epochs\n        )\n\n        optimizer = AdamW(\n            optimizer_parameters,\n            lr=args.learning_rate * xm.xrt_world_size()\n        )\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=0,\n            num_training_steps=num_train_steps\n        )\n\n        xm.master_print("Training is Starting.....")\n        best_jac = -1000\n\n        for epoch in range(args.epochs):\n            para_loader = pl.ParallelLoader(train_loader, [args.device])\n            train_loss = train(\n                args, \n                para_loader.per_device_loader(args.device),\n                model, \n                optimizer,\n                scheduler,\n                epoch,\n                f\n            )\n            para_loader = pl.ParallelLoader(valid_loader, [args.device])\n            valid_jac = valid(\n                args, \n                para_loader.per_device_loader(args.device),\n                model, \n                tokenizer,\n                epoch,\n                f\n            )\n\n            jac = xm.mesh_reduce("jac_reduce", valid_jac, reduce_fn)\n            xm.master_print(f"**** Epoch {epoch+1} **==>** Jaccard = {jac}")\n\n            log_ = f"**** Epoch {epoch+1} **==>** Jaccard = {jac}"\n\n            f.write(log_ + "\\n\\n")\n\n            \n\n            if jac > best_jac:\n                xm.master_print("**** Model Improved !!!! Saving Model")\n                xm.save(model.state_dict(), os.path.join(args.save_path, f"fold_{args.fold_index}"))\n                best_jac = jac\n\n\n    \n    def _mp_fn(rank, flags):\n        torch.set_default_tensor_type(\'torch.FloatTensor\')\n        a = run()\n    \n    FLAGS={}\n    xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=1, start_method=\'fork\')\n\n\nif __name__ == "__main__":\n    main()')




get_ipython().system('wget https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model')




get_ipython().system('python XLMROBERTA_TPU.py --fold_index=0                   --model_path="xlm-roberta-base"                   --spt_path="xlm-roberta-base-sentencepiece.bpe.model"                   --output_dir="xlm-roberta-base"                   --exp_name="xlm-roberta-base"                   --batch_size=64')




get_ipython().system('python XLMROBERTA_TPU.py --fold_index=1                   --model_path="xlm-roberta-base"                   --spt_path="albert-xlarge-v2-spiece.model"                   --output_dir="xlm-roberta-base"                   --exp_name="xlm-roberta-base"                   --batch_size=64')




get_ipython().system('python XLMROBERTA_TPU.py --fold_index=2                   --model_path="xlm-roberta-base"                   --spt_path="albert-xlarge-v2-spiece.model"                   --output_dir="xlm-roberta-base"                   --exp_name="xlm-roberta-base"                   --batch_size=64')






